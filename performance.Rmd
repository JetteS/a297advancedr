# Performance

```{r, include=FALSE}
source("knitr-options.R")
source("spelling-check.R")
```

The people who say that R is just always slow usually are not great R programmers. It is true that writing inefficient R code is easy, yet writing efficient R code is also possible when you know what you're doing. 
In this chapter, you will learn how to write R(cpp) code that is fast.

Some years ago, when introducing the Julia language, some really unfair benchmark comparisons were released (e.g. with loops where obvious and simpler vectorized code would have been much faster). I'm still mad at Julia for this and [I am not the only one](https://matloff.wordpress.com/2014/05/21/r-beats-python-r-beats-julia-anyone-else-wanna-challenge-r/).

## Early advice

### NEVER GROW A VECTOR

Example computing the cumulative sums of a vector:

```{r}
x <- rnorm(1e4)  # Try also with n = 1e5
system.time({
  current_sum <- 0
  res <- c()
  for (x_i in x) {
    current_sum <- current_sum + x_i
    res <- c(res, current_sum)
  }
})
```

Here, at each iterating, you reallocating a vector (of increasing size). It makes your computation quadratic with the size of `x` (if you multiply the size by 2, you can expect the execution time to be multiplied by 4, for large sample sizes), whereas it should be only linear. Indeed, we will see that execution time can be composed of computation time but also allocation time.

A good solution is to always pre-allocating your results (if you know the size):

```{r}
system.time({
  current_sum <- 0
  res2 <- double(length(x))
  for (i in seq_along(x)) {
    current_sum <- current_sum + x[i]
    res2[i] <- current_sum
  }
})
all.equal(res2, res)
```

An even better solution would be to avoid the loop by using a vectorized function:

```{r}
system.time(res3 <- cumsum(x))
all.equal(res3, res)
x <- rnorm(1e7)
system.time(cumsum(x))
```

As a second example, let us generate a matrix of uniform values (max changing for every column):

```{r}
n <- 1e3
max <- 1:1000
system.time({
  mat <- NULL
  for (m in max) {
    mat <- cbind(mat, runif(n, max = m))
  }
})
apply(mat, 2, max)[1:10]
```

So, we can either pre-allocate a list or a matrix:

```{r}
system.time({
  l <- vector("list", length(max))
  for (i in seq_along(max)) {
    l[[i]] <- runif(n, max = max[i])
  }
  mat2 <- do.call("cbind", l)
})
apply(mat2, 2, max)[1:10]
```

```{r}
system.time({
  mat3 <- matrix(0, n, length(max))
  for (i in seq_along(max)) {
    mat3[, i] <- runif(n, max = max[i])
  }
})
apply(mat3, 2, max)[1:10]
```

Instead of pre-allocating yourself, you can use `sapply` (or `lapply` and calling `do.call()` after, as previously done):

```{r}
system.time(
  mat4 <- sapply(max, function(m) runif(n, max = m))
)
apply(mat4, 2, max)[1:10]
```

### Access columns of a matrix

When you do computations on a matrix, recall that a matrix is just a big vectors with some dimensions.

```{r}
vec <- 1:20
dim(vec) <- c(4, 5)
vec
```

So, as you can see in this example, R matrices are column-oriented, which means that elements of the same column are stored contiguously in memory and are accessed faster together. 


### Use the right function

Often, to optimize your code, you can simply find the right function in the right package to do what you need to do. 

For example, using `rowMeans(x)` instead of `apply(x, 1, mean)` can save you a lot of time. If you want more efficient functions that apply to rows and columns of matrices, you can check [package **matrixStats**](https://github.com/HenrikBengtsson/matrixStats). 

Another example is when reading large text files. In such cases, prefer using `data.table::fread()` rather than `read.table()`.

Generally, packages that uses C/Rcpp are efficient.

### Do not try to optimize everything

> The real problem is that programmers have spent far too much time worrying about efficiency in the wrong places and at the wrong times; premature optimization is the root of all evil (or at least most of it) in programming.
> 
> -- Donald Knuth

If you try to optimize each and every part of your code, you will end up losing a lot of time writing it and it will probably less readable.

R is great to prototyping quickly because you can write code in a concise and easy way. Begin by doing just that. If performance matters, then profile your code to see which part of your code is taking too much time and optimize only this part!

Learn more on how to profile your code in RStudio in [this article](https://support.rstudio.com/hc/en-us/articles/218221837-Profiling-with-RStudio).

<!-- ```{r, echo=FALSE} -->
<!-- knitr::include_graphics("https://twitter.com/twitter/statuses/874522268331671557") -->
<!-- ``` -->

## Vectorization

See [this great blog post by Noam Ross](http://www.noamross.net/blog/2014/4/16/vectorization-in-r--why.html) to understand vectorization. 

### Exercise

Monte-Carlo integration (example from [book Efficient R programming](https://bookdown.org/csgillespie/efficientR/programming.html#vectorised-code))

Suppose we wish to estimate the integral $\int_0^1 x^2 dx$ using a Monte-Carlo method. Essentially, we throw darts at the curve and count the number of darts that fall below the curve (as in the following figure).

```{r monte-carlo, echo=FALSE}
knitr::include_graphics("https://bookdown.org/csgillespie/efficientR/_main_files/figure-html/3-1-1.png")
```

_Monte Carlo Integration pseudo-code_

1. Initialize: `hits = 0`
1. __for i in 1:N__
1. $~~$ Generate two random numbers, $U_1$ and $U_2$, between 0 and 1
1. $~~$ If $U_2 < U_1^2$, then `hits = hits + 1`
1. __end for__
1. Area estimate = `hits/N`

Naively implementing this Monte-Carlo algorithm in R would typically lead to something like:

```{r}
monte_carlo <- function(N) {
  
  hits <- 0
  for (i in seq_len(N)) {
    u1 <- runif(1)
    u2 <- runif(1)
    if (u1 ^ 2 > u2) {
      hits <- hits + 1
    }
  }
  
  hits / N
}
```

This takes a few seconds for `N = 1e6`:

```{r cache=TRUE}
N <- 1e6
system.time(monte_carlo(N))
```

**Your task: Find a vectorized solution for this problem:**

```{r echo=FALSE}
monte_carlo_vec <- function(N) mean(runif(N)^2 > runif(N))
```

```{r}
system.time(monte_carlo_vec(N))
```


## Algorithms & data structures

Sometimes, getting the right data structure (e.g. using a matrix instead of a data frame or integers instead of characters) can save you some computation time.

Is your algorithm doing some redundant computations making it e.g. quadratic instead of linear with respect to the dimension of your data?

See exercises (section \@ref(exos)) for some insights.

## Rcpp

## Parallel

I basically always use `foreach` and recommend to do so. See [this guide to parallelism in R with `foreach`](https://privefl.github.io/blog/a-guide-to-parallelism-in-r/). 

Just remember to optimize your code before trying to parallelize it. 


## Linear algebra

In R, prefer using `crossprod(X)` and `tcrossprod(X)` instead of `t(X) %*% X` and `X %*% t(X)`. Also using `A %*% (B %*% y)` and `solve(A, y)` will be faster than `A %*% B %*% y` and `solve(A) %*% y`.

Don't re-implement linear algebra such as matrix products yourself. There exists some highly optimized libraries for this. If you want to use linear algebra in Rcpp, try [RcppArmadillo](http://dirk.eddelbuettel.com/code/rcpp.armadillo.html) or [RcppEigen](http://dirk.eddelbuettel.com/code/rcpp.eigen.html).

If you want to use some optimized multi-threaded linear library, install [Microsoft R Open](https://mran.revolutionanalytics.com/documents/rro/multithread). 


## Exercises {#exos}

Generate $10^8$ (begin with $10^5$) steps of the process described by the formula:$$X(0)=0$$$$X(t+1)=X(t)+Y(t)$$ where $Y(t)$ are independent random variables with the distribution $N(0,1)$. Then, calculate in what percentage of indices $t$ the value of $X(t)$ was negative. You don't need to store values of $X$ if you don't want to.
What would be the benefit of writing an Rcpp function over a simple vectorized R function?

```{r}
set.seed(1)
system.time(p <- advr38pkg::random_walk_neg_prop(1e7))
p
```

***

```{r}
mat <- as.matrix(mtcars)
ind <- seq_len(nrow(mat))
mat_big <- mat[rep(ind, 1000), ]  ## 1000 times bigger dataset
last_row <- mat_big[nrow(mat_big), ]
```

Speed up these loops:

```{r}
system.time({
  for (j in 1:ncol(mat_big)) {
    for (i in 1:nrow(mat_big)) {
      mat_big[i, j] <- 10 * mat_big[i, j] * last_row[j]
    }
  }
})
```

***

Why `colSums()` on a whole matrix is faster than on only half of it?

```{r}
m0 <- matrix(rnorm(1e6), 1e3, 1e3)
microbenchmark::microbenchmark(
  colSums(m0[, 1:500]), 
  colSums(m0)
)
```

***

Try to speed up this code by vectorizing it first. Then, recode it in Rcpp and benchmark all the solutions you came up with.

```{r}
M <- 50
step1 <- runif(M)
A <- rnorm(M)
N <- 1e4

tau <- matrix(0, N + 1, M)
tau[1, ] <- A
for (j in 1:M) {
  for (i in 2:nrow(tau)) {
    tau[i, j] <- tau[i - 1, j] + step1[j] * 1.0025^(i - 2)
  }
} 
```

***

Make a fast function that counts the number of elements between a sequence of breaks. Can you do it in base R? Try also implementing it in Rcpp. How can you implement a solution whose computation time doesn't depend on the number of breaks? [Which are the special cases that you should consider?]

```{r}
x <- sample(10, size = 1e4, replace = TRUE)
breaks <- c(1, 3, 9, 9.5, 10)
table(cut(x, breaks))
hist(x, breaks, plot = FALSE)$counts  # include first break
advr38pkg::count_by_breaks(x, breaks)
advr38pkg::count_by_breaks_fast(x, breaks)

microbenchmark::microbenchmark(
  table(cut(x, breaks)), 
  hist(x, breaks, plot = FALSE)$counts, 
  advr38pkg::count_by_breaks(x, breaks),
  advr38pkg::count_by_breaks_fast(x, breaks)
)

x2 <- sample(10, size = 1e5, replace = TRUE)
breaks2 <- seq(0, 10, length.out = 100)
microbenchmark::microbenchmark(
  advr38pkg::count_by_breaks(x2, breaks),
  advr38pkg::count_by_breaks_fast(x2, breaks),
  advr38pkg::count_by_breaks(x2, breaks2),
  advr38pkg::count_by_breaks_fast(x2, breaks2)
)
```

***

An user wants to implement some sampling on a sparse matrix and provide this code:

```{r, eval=FALSE}
library(Matrix)
N <- 100
m <- Matrix(0, nrow = N, ncol = N)

for (j in 1:N) {
  cols <- sample((1:N)[-j], 2)  # 2 columns != j 
  m[j, cols] <- 1
}
```

This code is slow; can you find two major reasons why? 

How can you more efficiently assign 1s? Can you use sampling with replacement (which can be easily vectorized) in this example?

***

Make a fast function that returns all prime numbers up to a number `N`.

```{r}
N <- 1e6
system.time(
  primes <- advr38pkg::AllPrimesUpTo(N)
)
plot(primes, pch = 20, cex = 0.5)
```

***

Imagine you have a list of animals, which are infected by other individuals:

```{r}
# Make some data
allanimals <- data.frame(
  AnimalID = c("a1", "a2", "a3", "a4", "a5", "a6", "a7", "a8", "b1", "b2", "b3",
               "b4", "b5", "c1", "c2", "c3", "c4", "d1", "d2", "e1", "e2", "e3",
               "e4", "e5", "e6", "f1", "f2", "f3", "f4", "f5", "f6", "f7"),
  InfectingAnimal = c(NA, "a1", "a2", "a3", "a4", "a5", "a6", "a7", "a2", "b1",
                      "b2", "b3", "b4", "b3", "c1", "c2", "c3", "c3", "d1",
                      "b1", "e1", "e2", "e3", "e4", "e5", "e1", "f1", "f2", 
                      "f3", "f4", "f5", "f6"), 
  habitat = c(1L, 2L, 1L, 2L, 2L, 1L, 3L, 2L, 4L, 5L, 6L, 1L, 2L, 3L, 2L, 3L, 
              2L, 1L, 1L, 2L, 5L, 4L, 1L, 1L, 1L, 1L, 4L, 5L, 4L, 5L, 4L, 3L),
  stringsAsFactors = FALSE
)
# Check it out
DT::datatable(allanimals)
```

For a given animal (1), you want to get which animal (2) infected (1), and then which animal infected (3), and so on you want to get the whole path of infection. 

```{r, echo=FALSE, out.width="20%"}
knitr::include_graphics("https://i.stack.imgur.com/oWoOc.jpg")
```

For example for animal `d2`, you want to return: 

```{r, echo=FALSE}
allanimals_ID <- unique(c(allanimals$AnimalID, allanimals$InfectingAnimal))

infected <- rep(NA_integer_, length(allanimals_ID))
infected[match(allanimals$AnimalID, allanimals_ID)] <-
  match(allanimals$InfectingAnimal, allanimals_ID)

curOne <- match("d2", allanimals_ID)
path <- list()
i <- 1
while (!is.na(nextOne <- infected[curOne])) {
  path[[i]] <- curOne
  i <- i + 1
  curOne <- nextOne
}

allanimals[unlist(path), ]
```

Find an efficient solution to this problem so that your solution could be used for a large dataset.

***

Find a fast method to compute pairwise distances between 2 matrices. A naive R function would be:

```{r}
naive_pdist <- function(A, B) {
  # A: matrix with observation vectors (nrow = number of observations)
  # B: matrix with another set of vectors (e.g. cluster centers)
  result = matrix(ncol = nrow(B), nrow = nrow(A))
  for (i in 1:nrow(A))
      for (j in 1:nrow(B))
          result[i,j] = sqrt(sum( (A[i,] - B[j,])^2 ))

  result
}
```

To see a comparison of different computation strategies, see [this nice blog post](http://blog.felixriedel.com/2013/05/pairwise-distances-in-r/).
