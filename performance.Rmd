---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Performance

```{r, include=FALSE}
source("knitr-options.R")
WORDS_TO_IGNORE <- c("truelength", "growable", "precomputing", "vectorize")
source("spelling-check.R")
```

Some resources used here or for further reading:

- [Advanced R](https://adv-r.hadley.nz/perf-improve.html)
- [Efficient R programming](https://bookdown.org/csgillespie/efficientR/)

The people who say that "R is just always slow" are usually not great R programmers. It is true that writing inefficient R code is easy, yet writing efficient R code is also possible when you know what you're doing. 
In this chapter, you will learn how to write R(cpp) code that is fast.

<!-- Some years ago, when introducing the Julia language, some really unfair benchmark comparisons were released (e.g. with loops where obvious and simpler vectorized code would have been much faster). I'm still mad at Julia for this and [I am not the only one](https://matloff.wordpress.com/2014/05/21/r-beats-python-r-beats-julia-anyone-else-wanna-challenge-r/). -->

## R's memory management

See [this chapter of Advanced R](https://adv-r.hadley.nz/names-values.html).

## Early advice

### NEVER GROW A VECTOR

Example computing the cumulative sums of a vector:

```{r}
x <- rnorm(2e4)  # Try also with n = 1e5
system.time({
  current_sum <- 0
  res <- c()
  for (x_i in x) {
    current_sum <- current_sum + x_i
    res <- c(res, current_sum)
  }
})
```

Here, at each iteration, you are reallocating a vector (of increasing size). Allocation of memory takes time as well as computations. This makes your code quadratic with the size of `x` (if you multiply the size of `x` by 2, you can expect the execution time to be multiplied by 4, for large sample sizes), whereas it should be only linear. 

A good solution is to always pre-allocate your results (if you know the size):

```{r}
system.time({
  current_sum <- 0
  res2 <- double(length(x))
  for (i in seq_along(x)) {
    current_sum <- current_sum + x[i]
    res2[i] <- current_sum
  }
})
all.equal(res2, res)
```

If you don't know the size of the results, you can store them in a list and merge them afterwards:

```{r}
system.time({
  current_sum <- 0
  res3 <- list()
  for (i in seq_along(x)) {
    current_sum <- current_sum + x[i]
    res3[[i]] <- current_sum
  }
})
all.equal(unlist(res3), res)
```

With recent versions of R (>= 3.4), you can efficiently grow a vector using

```{r}
system.time({
  current_sum <- 0
  res4 <- c()
  for (i in seq_along(x)) {
    current_sum <- current_sum + x[i]
    res4[i] <- current_sum
  }
})
all.equal(res4, res)
```

> Assigning to an element of a vector beyond the current length now over-allocates by a small fraction. The new vector is marked internally as growable, and the true length of the new vector is stored in the truelength field. This makes building up a vector result by assigning to the next element beyond the current length more efficient, though pre-allocating is still preferred. The implementation is subject to change and not intended to be used in packages at this time. ([NEWS](https://cran.r-project.org/doc/manuals/r-release/NEWS.html))

An even better solution would be to avoid the loop by using a vectorized function:

```{r}
system.time(res5 <- cumsum(x))
all.equal(res5, res)
x <- rnorm(1e7)
system.time(cumsum(x))
```

***

As a second example, let us generate a matrix of uniform values (max changing for every column):

```{r}
n <- 1e3
max <- 1:1000
system.time({
  mat <- NULL
  for (m in max) {
    mat <- cbind(mat, runif(n, max = m))
  }
})
apply(mat, 2, max)[1:10]
```

Instead, we should pre-allocate a matrix of the right size:

```{r}
system.time({
  mat3 <- matrix(0, n, length(max))
  for (i in seq_along(max)) {
    mat3[, i] <- runif(n, max = max[i])
  }
})
apply(mat3, 2, max)[1:10]
```

Or we could use a list instead. What is nice with using a list is that you don't need to pre-allocate. Indeed, as opposed to atomic vectors, each element of a list is in different places in memory so that you don't have to reallocate all the data when you add an element to a list.

```{r}
system.time({
  l <- list()
  for (i in seq_along(max)) {
    l[[i]] <- runif(n, max = max[i])
  }
  mat4 <- do.call("cbind", l)
})
apply(mat4, 2, max)[1:10]
```

Instead of pre-allocating yourself, you can use `sapply` (or `lapply` and calling `do.call()` after, as previously done):

```{r}
system.time(
  mat4 <- sapply(max, function(m) runif(n, max = m))
)
apply(mat4, 2, max)[1:10]
```

Don't listen to people telling you that `sapply()` is a vectorized operation that is so much faster than loops. You can learn more with [this blog post](https://privefl.github.io/blog/why-loops-are-slow-in-r/).

<!-- ### Access columns of a matrix -->

<!-- When you do computations on a matrix, recall that a matrix is just a vector with some dimensions. -->

<!-- ```{r} -->
<!-- vec <- 1:20 -->
<!-- dim(vec) <- c(4, 5) -->
<!-- vec -->
<!-- ``` -->

<!-- So, as you can see in this example, R matrices are column-oriented, which means that elements of the same column are stored contiguously in memory. Therefore, accessing elements of the same column is fast.  -->


### Use the right function

Often, in order to optimize your code, you can simply find the right function to do what you need to do. 

For example, `rowMeans(x)` is much faster than `apply(x, 1, mean)`. 
Similarly, if you want more efficient functions that apply to rows and columns of matrices, you can check [package {matrixStats}](https://github.com/HenrikBengtsson/matrixStats). 

Another example is when reading large text files; in such cases, prefer using `data.table::fread()` rather than `read.table()`.

Generally, packages that uses C/Rcpp are efficient.

### Do not try to optimize everything

> "Programmers waste enormous amounts of time thinking about, or worrying
> about, the speed of noncritical parts of their programs, and these attempts 
> at efficiency actually have a strong negative impact when debugging and
> maintenance are considered."
>
> --- Donald Knuth.

If you try to optimize each and every part of your code, you will end up losing a lot of time writing it, and it will probably make your code less readable.

R is great at prototyping quickly because you can write code in a concise and easy way. Start by doing just that. If performance matters, then profile your code to see which part of your code is taking too much time and optimize only this part!

Learn more on how to profile your code in RStudio in [this article](https://support.rstudio.com/hc/en-us/articles/218221837-Profiling-R-code-with-the-RStudio-IDE).


## Vectorization

See [this great blog post by Noam Ross](http://www.noamross.net/blog/2014/4/16/vectorization-in-r--why.html) to understand vectorization. 

### Exercise

Monte-Carlo integration (example from [book Efficient R programming](https://bookdown.org/csgillespie/efficientR/programming.html#vectorised-code))

Suppose we wish to estimate the integral $\int_0^1 x^2 dx$ using a Monte-Carlo method. Essentially, we throw darts at the curve and count the number of darts that fall below the curve (as in the following figure).

```{r monte-carlo, echo=FALSE}
knitr::include_graphics("https://bookdown.org/csgillespie/efficientR/_main_files/figure-html/3-1-1.png")
```

_Monte Carlo Integration pseudo-code_

1. Initialize: `hits = 0`
1. __for i in 1:N__
1. $~~$ Generate two random numbers, $U_1$ and $U_2$, between 0 and 1
1. $~~$ If $U_2 < U_1^2$, then `hits = hits + 1`
1. __end for__
1. Area estimate = `hits / N`

Naively implementing this Monte-Carlo algorithm in R would typically lead to something like:

```{r}
monte_carlo <- function(N) {
  
  hits <- 0
  for (i in seq_len(N)) {
    u1 <- runif(1)
    u2 <- runif(1)
    if (u1 ^ 2 > u2) {
      hits <- hits + 1
    }
  }
  
  hits / N
}
```

This takes a few seconds for `N = 1e6`:

```{r cache=TRUE}
N <- 1e6
system.time(monte_carlo(N))
```

Your task: find a vectorized solution for this problem:

```{r echo=FALSE}
monte_carlo_vec <- function(N) mean(runif(N)^2 > runif(N))
```

```{r}
system.time(monte_carlo_vec(N))
```

## Rcpp {#Rcpp}

See [this presentation](https://privefl.github.io/R-presentation/Rcpp.html).

You have this data and this working code (a loop) that is slow 

```{r, eval=FALSE}
mydf <- readRDS(system.file("extdata/one-million.rds", package = "advr38pkg"))

QRA_3Dmatrix <- array(0, dim = c(max(mydf$ID), max(mydf$Volume), 2))

for (i in seq_len(nrow(mydf))) {
  # Row corresponds to the ID class
  row    <- mydf$ID[i]
  # Column corresponds to the volume class
  column <- mydf$Volume[i]
  # Number of events, initially zero, then +1
  QRA_3Dmatrix[row, column, 1] <- QRA_3Dmatrix[row, column, 1] + 1  
  # Sum energy 
  QRA_3Dmatrix[row, column, 2] <- QRA_3Dmatrix[row, column, 2] + 
    1 - 1.358 / (1 + exp( (1000 * mydf$Energy[i] - 129000) / 120300 ))
}
```

Rewrite this for-loop with Rcpp. 

You can also try to use {dplyr} for this problem.


## Linear algebra

In R, prefer using `crossprod(X)` and `tcrossprod(X)` instead of `t(X) %*% X` and `X %*% t(X)`. Also using `A %*% (B %*% y)` and `solve(A, y)` will be faster than `A %*% B %*% y` and `solve(A) %*% y`.

Don't re-implement linear algebra operations (such as matrix products) yourself. There exist some highly optimized libraries for this. If you want to use linear algebra in Rcpp, try [RcppArmadillo](http://dirk.eddelbuettel.com/code/rcpp.armadillo.html) or [RcppEigen](http://dirk.eddelbuettel.com/code/rcpp.eigen.html).

If you want to use some optimized multi-threaded linear library, you can try [Microsoft R Open](https://mran.revolutionanalytics.com/documents/rro/multithread). 

### Exercises

Compute the Euclidean distances between each of row of `X` and each row of `Y`:

```{r}
set.seed(1)
X <- matrix(rnorm(1000), ncol = 5)
Y <- matrix(rnorm(5000), ncol = 5)
```

A naive implementation would be:

```{r}
system.time({
  dist <- matrix(NA_real_, nrow(X), nrow(Y))
  for (i in seq_len(nrow(X))) {
    for (j in seq_len(nrow(Y))) {
      dist[i, j] <- sqrt(sum((X[i, ] - Y[j, ])^2))
    }
  }
})
```

Try first to remove one of the two loops using `sweep()` instead. Then, try to implement a fully vectorized solution based on this hint: $\text{dist}(X_i, Y_j)^2 = (X_i - Y_j)^T (X_i - Y_j) = X_i^T X_i + Y_j^T Y_j - 2 X_i^T Y_j$.

A faster possible solution takes

```{r, echo=FALSE}
fast_dist <- function(x, y) {
  sqrt(outer(rowSums(x^2), rowSums(y^2), '+') - tcrossprod(x, 2 * y))
}
stopifnot(all.equal(fast_dist(X, Y), dist))
system.time(fast_dist(X, Y))
```

<!-- *** -->

<!-- Rewrite this problem to use linear algebra instead of a loop (Hint: resize the 3-dimensional arrays as 2D matrices): -->

<!-- ```{r} -->
<!-- N <- 1e5 -->
<!-- x <- rnorm(N*3*3);   dim(x) <- c(N,3,3) -->
<!-- y <- rnorm(N*3*3);   dim(y) <- c(N,3,3) -->

<!-- system.time({ -->
<!--   gg <- 0 -->
<!--   for (n in 1:dim(x)[1]){ -->
<!--     gg <- gg + t(x[n,,]) %*% y[n,,] -->
<!--   } -->
<!-- }) -->
<!-- ``` -->

<!-- A possible faster solution takes -->

<!-- ```{r, echo=FALSE} -->
<!-- system.time({ -->
<!--   dim(x) <- c(3 * N, 3) -->
<!--   dim(y) <- c(3 * N, 3) -->
<!--   gg2 <- crossprod(x, y) -->
<!-- }) -->
<!-- ``` -->


## Algorithms & data structures

Sometimes, getting the right data structure (e.g. using a matrix instead of a data frame or integers instead of characters) can save you some computation time.

Is your algorithm doing some redundant computations making it e.g. quadratic instead of linear with respect to the dimension of your data?

See exercises (section \@ref(exos)) for some insights.

You can also find a detailed example in [this blog post](https://privefl.github.io/blog/performance-when-algorithmics-meets-mathematics/).

## Exercises {#exos}

Generate $10^7$ (start with $10^5$) steps of the process described by the formula:$$X(0)=0$$$$X(t+1)=X(t)+Y(t)$$ where $Y(t)$ are independent random variables with the distribution $N(0,1)$. 
Then, calculate the percentage of $X(t)$ that are negative. 
You do not need to store all values of $X$.

A naive implementation with a for-loop could be:

```{r}
set.seed(1)

system.time({
  N <- 1e5
  x <- 0
  count <- 0
  for (i in seq_len(N)) {
    y <- rnorm(1)
    x <- x + y
    if (x < 0) count <- count + 1
  }
  p <- count / N
})
p
```

Try to vectorize this after having written the value of X(0), X(1), X(2), and X(3).
What would be the benefit of writing an Rcpp function over a simple vectorized R function?

```{r}
set.seed(1)
system.time(p2 <- advr38pkg::random_walk_neg_prop(1e5))
p2
```

```{r}
set.seed(1)
system.time(p3 <- advr38pkg::random_walk_neg_prop(1e7))
p3
```

***

```{r}
mat <- as.matrix(mtcars)
ind <- seq_len(nrow(mat))
mat_big <- mat[rep(ind, 1000), ]  ## 1000 times bigger dataset
last_row <- mat_big[nrow(mat_big), ]
```

Speed up these loops (vectorize):

```{r}
system.time({
  for (j in 1:ncol(mat_big)) {
    for (i in 1:nrow(mat_big)) {
      mat_big[i, j] <- 10 * mat_big[i, j] * last_row[j]
    }
  }
})
```

***

Why `colSums()` on a whole matrix is faster than on only half of it?

```{r}
m0 <- matrix(rnorm(1e6), 1e3, 1e3)
microbenchmark::microbenchmark(
  colSums(m0[, 1:500]), 
  colSums(m0)
)
```

***

Try to speed up this code by vectorizing it first, and/or by precomputing. Then, recode it in Rcpp and benchmark all the solutions you came up with.

```{r}
M <- 50
step1 <- runif(M)
A <- rnorm(M)
N <- 1e4

tau <- matrix(0, N + 1, M)
tau[1, ] <- A
for (j in 1:M) {
  for (i in 2:nrow(tau)) {
    tau[i, j] <- tau[i - 1, j] + step1[j] * 1.0025^(i - 2)
  }
} 
```

***

Make a fast function that counts the number of elements between a sequence of breaks. Can you do it in base R? Try also implementing it in Rcpp. How can you implement a solution whose computation time doesn't depend on the number of breaks? [Which are the special cases that you should consider?]

```{r}
x <- sample(10, size = 1e4, replace = TRUE)
breaks <- c(1, 3, 8.5, 9.5, 10)
table(cut(x, breaks), exclude = NULL) # does not include first break (1)
hist(x, breaks, plot = FALSE)$counts  # includes first break
advr38pkg::count_by_breaks(x, breaks)
advr38pkg::count_by_breaks_fast(x, breaks)

microbenchmark::microbenchmark(
  table(cut(x, breaks)), 
  hist(x, breaks, plot = FALSE)$counts, 
  advr38pkg::count_by_breaks(x, breaks, use_outer = TRUE),
  advr38pkg::count_by_breaks(x, breaks, use_outer = FALSE),
  advr38pkg::count_by_breaks_fast(x, breaks)
)

x2 <- sample(100, size = 1e5, replace = TRUE)
breaks2 <- breaks * 10
breaks3 <- seq(0, 100, length.out = 100)
microbenchmark::microbenchmark(
  advr38pkg::count_by_breaks(x2, breaks2),
  advr38pkg::count_by_breaks_fast(x2, breaks2),
  advr38pkg::count_by_breaks(x2, breaks3),
  advr38pkg::count_by_breaks_fast(x2, breaks3)
)
```

***

An R user wants to implement some sampling on a sparse matrix and provides this working code:

```{r}
N <- 2000
system.time({
  m <- Matrix::Matrix(0, nrow = N, ncol = N)
  for (j in 1:N) {
    cols <- sample((1:N)[-j], 2)  # pick 2 columns that are not j 
    m[j, cols] <- 1
  }
})
```

This code is slow; can you find two major reasons why? 

How can you more efficiently assign 1s? A faster solution would take:

```{r, echo=FALSE}
system.time({
  m <- Matrix::Matrix(0, nrow = N, ncol = N)
  ind <- sapply(1:N, function(j) sample((1:N)[-j], 2))
  m[t(ind)] <- 1
})
```

Can you use sampling with replacement (to avoid unnecessarily allocating memory) in this example? A faster solution would take:

```{r, echo=FALSE}
sample2 <- function(N) {
  
  res <- rep(NA_integer_, 2 * N)
  
  for (j in seq_len(N)) {
    
    # sample first one
    repeat {
      ind1 <- sample(N, 1)
      if (ind1 != j) break
    }
    
    # sample second one
    repeat {
      ind2 <- sample(N, 1)
      if (ind2 != j && ind2 != ind1) break
    }
    
    res[2 * j - 1:0] = c(ind1, ind2)
  }
  
  cbind(rep(1:N, each = 2), res)
}

system.time({
  m <- Matrix::Matrix(0, nrow = N, ncol = N)
  m[sample(N)] <- 1
})
```

It would be even faster using Rcpp (cf. [this SO answer](https://stackoverflow.com/a/45796424/6103040)).


***

Make a fast function that returns all prime numbers up to a number `N`.

```{r, dev='png'}
N <- 1e6
system.time(
  primes <- advr38pkg::AllPrimesUpTo(N)
)
plot(primes, pch = 20, cex = 0.5)
```

## Parallel

I basically always use `foreach` and recommend to do so. See [my guide to parallelism in R with `foreach`](https://privefl.github.io/blog/a-guide-to-parallelism-in-r/). 

**Just remember to optimize your code before trying to parallelize it.**

Try to parallelize some of your best solutions for the previous exercises.

